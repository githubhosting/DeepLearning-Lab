{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Program 11:\n","\n","#### Objective: \n","Implement the Self-Attention mechanism using the PyTorch framework.\n","\n","Tasks:\n","- Define the Self-Attention mechanism.\n","- Show the forward pass."]},{"cell_type":"code","execution_count":12,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-10T19:17:28.858098Z","iopub.status.busy":"2024-07-10T19:17:28.857646Z","iopub.status.idle":"2024-07-10T19:17:28.863867Z","shell.execute_reply":"2024-07-10T19:17:28.862635Z","shell.execute_reply.started":"2024-07-10T19:17:28.858063Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T19:17:29.687612Z","iopub.status.busy":"2024-07-10T19:17:29.687218Z","iopub.status.idle":"2024-07-10T19:17:29.701634Z","shell.execute_reply":"2024-07-10T19:17:29.700225Z","shell.execute_reply.started":"2024-07-10T19:17:29.687582Z"},"trusted":true},"outputs":[],"source":["class SelfAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads):\n","        super(SelfAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n","        self.head_dim = embed_dim // num_heads\n","        self.scale = self.head_dim ** -0.5\n","        \n","        self.query = nn.Linear(embed_dim, embed_dim)\n","        self.key = nn.Linear(embed_dim, embed_dim)\n","        self.value = nn.Linear(embed_dim, embed_dim)\n","        self.out = nn.Linear(embed_dim, embed_dim)\n","    \n","    def forward(self, x):\n","        batch_size, seq_len, embed_dim = x.size()\n","        \n","        # Compute Q, K, V matrices\n","        Q = self.query(x)\n","        K = self.key(x)\n","        V = self.value(x)\n","        \n","        # Split the embedding into multiple heads\n","        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","        \n","        # Compute attention scores\n","        attn_scores = torch.matmul(Q, K.transpose(-2,-1)) * self.scale\n","        attn_weights = F.softmax(attn_scores, dim=-1)\n","        \n","        # Compute the weighted sum of the values\n","        attn_output = torch.matmul(attn_weights, V)\n","        \n","        # Concatenate the multiple heads\n","        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n","        \n","        # Apply the final linear layer\n","        output = self.out(attn_output)\n","        \n","        return output"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T19:17:30.290816Z","iopub.status.busy":"2024-07-10T19:17:30.290321Z","iopub.status.idle":"2024-07-10T19:17:30.310840Z","shell.execute_reply":"2024-07-10T19:17:30.309507Z","shell.execute_reply.started":"2024-07-10T19:17:30.290778Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 10, 128])\n"]}],"source":["# Example usage\n","embed_dim = 128\n","num_heads = 8\n","seq_len = 10\n","batch_size = 32\n","\n","x = torch.randn(batch_size, seq_len, embed_dim)\n","self_attention = SelfAttention(embed_dim, num_heads)\n","output = self_attention(x)\n","print(output.shape)  # Output shape will be (batch_size, seq_len, embed_dim)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
