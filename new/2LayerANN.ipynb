{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Program 12:\n\n#### Objective: \nImplement a 2-layer Artificial Neural Network using NumPy.\n\nTasks:\n- Implement the forward pass of the network.\n- Implement the backward pass of the network.\n- Train the network.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\n# Input data and expected output\ninputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nexpected_output = np.array([[0], [1], [1], [0]])\n\n# Initialize weights and biases for the two layers\nnp.random.seed(0)  # For reproducibility\nweights1 = np.random.rand(2, 4)  # Weights for the first layer\nweights2 = np.random.rand(4, 1)  # Weights for the second layer\nbias1 = np.random.rand(1, 4)     # Bias for the first layer\nbias2 = np.random.rand(1, 1)     # Bias for the second layer\n\nlearning_rate = 0.1\n\n# Training loop\nfor epoch in range(10000):\n    # Forward pass\n\n    # First layer\n    hidden_layer_input = np.dot(inputs, weights1) + bias1\n    hidden_layer_output = sigmoid(hidden_layer_input)\n\n    # Second layer\n    final_output = sigmoid(np.dot(hidden_layer_output, weights2) + bias2)\n\n    # Backpropagation\n\n    # Calculate error\n    error = expected_output - final_output\n\n    # Compute gradients using chain rule\n    d_predicted_output = error * sigmoid_derivative(final_output)\n    error_hidden_layer = d_predicted_output.dot(weights2.T)\n    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n\n    # Update weights and biases\n    weights2 += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n    bias2 += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n    weights1 += inputs.T.dot(d_hidden_layer) * learning_rate\n    bias1 += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n\n    # Print the loss every 1000 epochs\n    if epoch % 1000 == 0:\n        print(f'Epoch {epoch} Loss: {np.mean(np.abs(error))}')\n\n# After training, print final outputs\nprint(\"Final outputs after training:\")\nprint(final_output)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-10T19:25:23.986597Z","iopub.execute_input":"2024-07-10T19:25:23.987058Z","iopub.status.idle":"2024-07-10T19:25:24.863978Z","shell.execute_reply.started":"2024-07-10T19:25:23.987020Z","shell.execute_reply":"2024-07-10T19:25:24.862736Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 0 Loss: 0.4962069130323263\nEpoch 1000 Loss: 0.4966696340133635\nEpoch 2000 Loss: 0.4873586889625524\nEpoch 3000 Loss: 0.44441845529227064\nEpoch 4000 Loss: 0.37097279982529396\nEpoch 5000 Loss: 0.21307364699933495\nEpoch 6000 Loss: 0.12377647992824192\nEpoch 7000 Loss: 0.08992923431098046\nEpoch 8000 Loss: 0.07261907603976829\nEpoch 9000 Loss: 0.06198650465344811\nFinal outputs after training:\n[[0.05454767]\n [0.94778145]\n [0.9473246 ]\n [0.05941317]]\n","output_type":"stream"}]}]}