{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Program 12:\n","\n","#### Objective: \n","Implement a 2-layer Artificial Neural Network using NumPy.\n","\n","Tasks:\n","- Implement the forward pass of the network.\n","- Implement the backward pass of the network.\n","- Train the network."]},{"cell_type":"code","execution_count":15,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-10T19:25:23.987058Z","iopub.status.busy":"2024-07-10T19:25:23.986597Z","iopub.status.idle":"2024-07-10T19:25:24.863978Z","shell.execute_reply":"2024-07-10T19:25:24.862736Z","shell.execute_reply.started":"2024-07-10T19:25:23.987020Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 Loss: 0.4962069130323263\n","Epoch 1000 Loss: 0.4966696340133635\n","Epoch 2000 Loss: 0.4873586889625524\n","Epoch 3000 Loss: 0.44441845529227064\n","Epoch 4000 Loss: 0.37097279982529396\n","Epoch 5000 Loss: 0.21307364699933495\n","Epoch 6000 Loss: 0.12377647992824192\n","Epoch 7000 Loss: 0.08992923431098046\n","Epoch 8000 Loss: 0.07261907603976829\n","Epoch 9000 Loss: 0.06198650465344811\n","Final outputs after training:\n","[[0.05454767]\n"," [0.94778145]\n"," [0.9473246 ]\n"," [0.05941317]]\n"]}],"source":["import numpy as np\n","\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","\n","inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","expected_output = np.array([[0], [1], [1], [0]])\n","\n","np.random.seed(0)\n","weights1 = np.random.rand(2, 4)\n","weights2 = np.random.rand(4, 1)\n","bias1 = np.random.rand(1, 4)\n","bias2 = np.random.rand(1, 1)\n","\n","learning_rate = 0.1\n","\n","for epoch in range(10000):\n","    hidden_layer_input = np.dot(inputs, weights1) + bias1\n","    hidden_layer_output = sigmoid(hidden_layer_input)\n","\n","    final_output = sigmoid(np.dot(hidden_layer_output, weights2) + bias2)\n","    error = expected_output - final_output\n","\n","    d_predicted_output = error * sigmoid_derivative(final_output)\n","    error_hidden_layer = d_predicted_output.dot(weights2.T)\n","    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n","    weights2 += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n","    bias2 += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n","    weights1 += inputs.T.dot(d_hidden_layer) * learning_rate\n","    bias1 += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n","\n","    if epoch % 1000 == 0:\n","        print(f\"Epoch {epoch} Loss: {np.mean(np.abs(error))}\")\n","\n","print(\"Final outputs after training:\")\n","print(final_output)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
